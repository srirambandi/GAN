{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "gan.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirambandi/GAN/blob/master/gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RTzsFdfJn69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e617b8f-a989-44f2-eb2e-e35770cd7a8d"
      },
      "source": [
        "# when running in colab notebooks, first install library\n",
        "!pip install import-ai\n",
        "# upload respective dataset manually from examples directory of the library or download as below\n",
        "!apt install subversion\n",
        "!svn checkout https://github.com/srirambandi/ai/trunk/examples/MNIST"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ai\n",
            "  Downloading https://files.pythonhosted.org/packages/16/c9/61a99b75a3ccd70ddd207ea12e28b9baa17feba9fffb2d7181d03d1b8147/import_ai-1.3.11-py3-none-any.whl\n",
            "Collecting graphviz>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/83/cc/c62100906d30f95d46451c15eb407da7db201e30f42008f3643945910373/graphviz-0.14-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.6/dist-packages (from import-ai) (1.18.5)\n",
            "Installing collected packages: graphviz, import-ai\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.14 import-ai-1.3.11\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1\n",
            "Suggested packages:\n",
            "  db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 subversion\n",
            "0 upgraded, 5 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 2,237 kB of archives.\n",
            "After this operation, 9,910 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libapr1 amd64 1.6.3-2 [90.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaprutil1 amd64 1.6.1-2 [84.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserf-1-1 amd64 1.3.9-6 [44.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsvn1 amd64 1.9.7-4ubuntu1 [1,183 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 subversion amd64 1.9.7-4ubuntu1 [834 kB]\n",
            "Fetched 2,237 kB in 1s (2,313 kB/s)\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../libapr1_1.6.3-2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.6.3-2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../libaprutil1_1.6.1-2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../libserf-1-1_1.3.9-6_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../libsvn1_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../subversion_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking subversion (1.9.7-4ubuntu1) ...\n",
            "Setting up libapr1:amd64 (1.6.3-2) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-2) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Setting up libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Setting up subversion (1.9.7-4ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "A    MNIST/test.npy\n",
            "A    MNIST/train.npy\n",
            "Checked out revision 120.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UW4HapEUesLd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dbac5a02-0423-4d8a-9eae-556c53e13372"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY0FMFMLJlgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ai\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNYfGAkfJlgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_dim = 100\n",
        "gf_dim = 64\n",
        "df_dim = 64"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiFy00xkJ3X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ai.manual_seed(2357)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ3zS_s-Jlgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(m):\n",
        "    train_dict = np.load('MNIST/train.npy', allow_pickle=True)\n",
        "    test_dict = np.load('MNIST/test.npy', allow_pickle=True)\n",
        "    data = np.concatenate([train_dict.item()['data'], test_dict.item()['data']])\n",
        "    data = data.transpose(1, 2, 0)   # making data batch-last\n",
        "    data = data.reshape(1, *data.shape) / 127.5 - 1.   # adding channel dimension and normalizing data\n",
        "    epoch = 4\n",
        "    \n",
        "    while True:\n",
        "        epoch += 1\n",
        "        for batch in range(int(data.shape[-1] / m)):\n",
        "            yield data[...,batch * m:(batch + 1) * m], epoch"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsEHzfzVJlgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(ai.Module):\n",
        "    def __init__(self):\n",
        "        self.g_fc = ai.Linear(z_dim, 8*gf_dim * 2 * 2)\n",
        "        self.g_bn1 = ai.BatchNorm((8*gf_dim, 2, 2))\n",
        "        self.g_deconv1 = ai.ConvTranspose2d(8*gf_dim, 4*gf_dim, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        self.g_bn2 = ai.BatchNorm((4*gf_dim, 4, 4))\n",
        "        self.g_deconv2 = ai.ConvTranspose2d(4*gf_dim, 2*gf_dim, kernel_size=5, stride=2, padding=2, a=0)\n",
        "        self.g_bn3 = ai.BatchNorm((2*gf_dim, 7, 7))\n",
        "        self.g_deconv3 = ai.ConvTranspose2d(2*gf_dim, gf_dim, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        self.g_bn4 = ai.BatchNorm((gf_dim, 14, 14))\n",
        "        self.g_deconv4 = ai.ConvTranspose2d(gf_dim, 1, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        \n",
        "    def forward(self, z):\n",
        "        o1 = ai.G.reshape(self.g_fc(z), (8*gf_dim, 2, 2))\n",
        "        o2 = ai.G.relu(self.g_bn1(o1))\n",
        "        o3 = ai.G.relu(self.g_bn2(self.g_deconv1(o2)))\n",
        "        o4 = ai.G.relu(self.g_bn3(self.g_deconv2(o3)))\n",
        "        o5 = ai.G.relu(self.g_bn4(self.g_deconv3(o4)))\n",
        "        fake_image = ai.G.tanh(self.g_deconv4(o5))\n",
        "        return fake_image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nG_7CSfJlgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(ai.Module):\n",
        "    def __init__(self):\n",
        "        self.d_conv1 = ai.Conv2d(1, 64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_conv2 = ai.Conv2d(64, 2*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn1 = ai.BatchNorm((2*64, 7, 7))\n",
        "        self.d_conv3 = ai.Conv2d(2*64, 3*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn2 = ai.BatchNorm((3*64, 4, 4))\n",
        "        self.d_conv4 = ai.Conv2d(3*64, 4*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn3 = ai.BatchNorm((4*64, 2, 2))\n",
        "        self.d_fc = ai.Linear(1024, 1)\n",
        "        \n",
        "    def forward(self, image):\n",
        "        o1 = ai.G.lrelu(self.d_conv1(image))\n",
        "        o2 = ai.G.lrelu(self.d_bn1(self.d_conv2(o1)))\n",
        "        o3 = ai.G.lrelu(self.d_bn2(self.d_conv3(o2)))\n",
        "        o4 = ai.G.lrelu(self.d_bn3(self.d_conv4(o3)))\n",
        "        o5 = self.d_fc(o4)\n",
        "        return ai.G.sigmoid(o5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l88SAS6rJlgu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "b2557575-a2e9-41a6-c3c2-1a409b2bf4f4"
      },
      "source": [
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "generator.load('/content/drive/My Drive/GAN/Generator.npy')\n",
        "discriminator.load('/content/drive/My Drive/GAN/Discriminator.npy')\n",
        "print(generator)\n",
        "print(discriminator)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model...\n",
            "Successfully loaded model from /content/drive/My Drive/GAN/Generator.npy\n",
            "loading model...\n",
            "Successfully loaded model from /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Generator(\n",
            "  g_fc: Linear(input_features=100, output_features=2048, bias=True)\n",
            "  g_bn1: BatchNorm((512, 2, 2), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv1: ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            "  g_bn2: BatchNorm((256, 4, 4), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv2: ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(0, 0), bias=True)\n",
            "  g_bn3: BatchNorm((128, 7, 7), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv3: ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            "  g_bn4: BatchNorm((64, 14, 14), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv4: ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            ")\n",
            "Discriminator(\n",
            "  d_conv1: Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_conv2: Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn1: BatchNorm((128, 7, 7), axis=-1, momentum=0.9, bias=True)\n",
            "  d_conv3: Conv2d(128, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn2: BatchNorm((192, 4, 4), axis=-1, momentum=0.9, bias=True)\n",
            "  d_conv4: Conv2d(192, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn3: BatchNorm((256, 2, 2), axis=-1, momentum=0.9, bias=True)\n",
            "  d_fc: Linear(input_features=1024, output_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md_IEa9CJlgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "L = ai.Loss(loss_fn='BCELoss')\n",
        "g_optim = ai.Optimizer(generator.parameters(), optim_fn='Adam', lr=lr, beta1=beta1)\n",
        "d_optim = ai.Optimizer(discriminator.parameters(), optim_fn='Adam', lr=lr, beta1=beta1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsfYfCeuJlg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it, epoch = 0, 0\n",
        "m = 64   # batch size\n",
        "n_discriminator = 1   # number of descriminator updates per generator update\n",
        "\n",
        "# real images data generator\n",
        "data = data_generator(m)\n",
        "\n",
        "sample_z = np.random.uniform(-1, 1, (z_dim, m))\n",
        "sampled_images = np.load('/content/drive/My Drive/GAN/sampled_images.npy')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-6QzGr9Jlg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampler(sampled_images):\n",
        "    ai.G.grad_mode = False\n",
        "\n",
        "    # generate images like real data\n",
        "    fake_images = generator.forward(sample_z).data\n",
        "    fake_images = (fake_images + 1.) / 2.\n",
        "\n",
        "    if sampled_images is not None:\n",
        "        sampled_images = np.concatenate([sampled_images, fake_images], axis=-1)\n",
        "    else:\n",
        "        sampled_images = fake_images\n",
        "    \n",
        "    ai.G.grad_mode = True\n",
        "\n",
        "    return sampled_images"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUKbYB9EJlg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a7f282c-1b04-4469-e6c7-165e80d0020f"
      },
      "source": [
        "for it in range(2801, 10000):\n",
        "    \n",
        "    # freeze generator before optimizing descriminator\n",
        "    for p in generator.parameters():\n",
        "        p.eval_grad = False\n",
        "\n",
        "    # training descriminator to identify real/fake data\n",
        "    for _ in range(n_discriminator):\n",
        "\n",
        "        real_images, epoch = data.__next__()\n",
        "        real_labels = np.ones((1, m))\n",
        "        if (real_images.shape[-1] != m):\n",
        "            continue\n",
        "\n",
        "        real_probs = discriminator.forward(real_images)\n",
        "        d_loss_real = L.loss(real_probs, real_labels)\n",
        "\n",
        "        z = np.random.randn(z_dim, m)\n",
        "        fake_images = generator.forward(z)\n",
        "        fake_labels =  np.zeros((1, m))\n",
        "\n",
        "        fake_probs = discriminator.forward(fake_images)\n",
        "        d_loss_fake = L.loss(fake_probs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.grad = np.zeros(d_loss.shape)\n",
        "\n",
        "        d_loss.backward()\n",
        "        d_optim.step()\n",
        "        d_optim.zero_grad()\n",
        "\n",
        "    # unfreeze generator\n",
        "    for p in generator.parameters():\n",
        "        p.eval_grad = True\n",
        "\n",
        "    # training generator to fool descriminator with fake data\n",
        "    z = np.random.uniform(-1, 1, (z_dim, m))\n",
        "    fake_images = generator.forward(z)\n",
        "    fake_labels =  np.ones((1, m))\n",
        "\n",
        "    fake_probs = discriminator.forward(fake_images)\n",
        "    g_loss = L.loss(fake_probs, fake_labels)\n",
        "\n",
        "    g_loss.backward()\n",
        "    g_optim.step()\n",
        "    g_optim.zero_grad()\n",
        "    d_optim.zero_grad()\n",
        "\n",
        "    if it%10 == 0:\n",
        "        print('Iter: {}, Epoch: {}, d_loss: {}, g_loss: {}'.format(it, epoch, d_loss.data[0, 0], g_loss.data[0, 0]))\n",
        "    \n",
        "    if it%100 == 0:\n",
        "        sampled_images=sampler(sampled_images)\n",
        "        np.save('/content/drive/My Drive/GAN/sampled_images.npy', sampled_images)\n",
        "        generator.save('/content/drive/My Drive/GAN/Generator.npy')\n",
        "        discriminator.save('/content/drive/My Drive/GAN/Discriminator.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using Adam\n",
            "using Adam\n",
            "Iter: 2810, Epoch: 5, d_loss: 0.27411479496428093, g_loss: 2.4400530971667953\n",
            "Iter: 2820, Epoch: 5, d_loss: 0.8274533723798714, g_loss: 2.880149383697795\n",
            "Iter: 2830, Epoch: 5, d_loss: 0.5192199908430898, g_loss: 3.7324129690541388\n",
            "Iter: 2840, Epoch: 5, d_loss: 0.22270328484357796, g_loss: 2.8303581993367586\n",
            "Iter: 2850, Epoch: 5, d_loss: 0.3099196196512733, g_loss: 2.7333496005067657\n",
            "Iter: 2860, Epoch: 5, d_loss: 0.14519277146068094, g_loss: 2.6926575244548525\n",
            "Iter: 2870, Epoch: 5, d_loss: 0.24841053183678202, g_loss: 3.9181068343728827\n",
            "Iter: 2880, Epoch: 5, d_loss: 0.5055866152650232, g_loss: 4.0973581859735315\n",
            "Iter: 2890, Epoch: 5, d_loss: 0.2655146366573576, g_loss: 3.0115939171749506\n",
            "Iter: 2900, Epoch: 5, d_loss: 0.26235743085060925, g_loss: 3.255875083124425\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 2910, Epoch: 5, d_loss: 0.32910723924670304, g_loss: 3.1753613742473044\n",
            "Iter: 2920, Epoch: 5, d_loss: 0.26140424706168663, g_loss: 3.0093581759330346\n",
            "Iter: 2930, Epoch: 5, d_loss: 0.25664860886123386, g_loss: 2.5633855432814427\n",
            "Iter: 2940, Epoch: 5, d_loss: 0.29906581990135056, g_loss: 2.9979257394272145\n",
            "Iter: 2950, Epoch: 5, d_loss: 0.35424591647663395, g_loss: 1.5127204991177787\n",
            "Iter: 2960, Epoch: 5, d_loss: 0.31431605367208704, g_loss: 3.663513241375263\n",
            "Iter: 2970, Epoch: 5, d_loss: 0.15704625100372077, g_loss: 2.4397595205724603\n",
            "Iter: 2980, Epoch: 5, d_loss: 0.26580857123540924, g_loss: 3.902251978027569\n",
            "Iter: 2990, Epoch: 5, d_loss: 0.3491323931647694, g_loss: 2.057766487142945\n",
            "Iter: 3000, Epoch: 5, d_loss: 0.4993501264763292, g_loss: 2.345384631615423\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3010, Epoch: 5, d_loss: 0.18922155344830208, g_loss: 2.7460449160005016\n",
            "Iter: 3020, Epoch: 5, d_loss: 0.2052902957484156, g_loss: 2.79768038345032\n",
            "Iter: 3030, Epoch: 5, d_loss: 0.5311282691926216, g_loss: 3.3743451342759854\n",
            "Iter: 3040, Epoch: 5, d_loss: 0.2763285832535217, g_loss: 2.6806024066338456\n",
            "Iter: 3050, Epoch: 5, d_loss: 0.33884517868964903, g_loss: 2.3240102217899588\n",
            "Iter: 3060, Epoch: 5, d_loss: 0.2530062957357465, g_loss: 3.4307818043534137\n",
            "Iter: 3070, Epoch: 5, d_loss: 0.9298758042960424, g_loss: 3.4107044364620465\n",
            "Iter: 3080, Epoch: 5, d_loss: 0.306832347986256, g_loss: 2.54385838242926\n",
            "Iter: 3090, Epoch: 5, d_loss: 0.37509817578036053, g_loss: 2.6025734534031613\n",
            "Iter: 3100, Epoch: 5, d_loss: 0.3255205613078114, g_loss: 3.6048026547078704\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3110, Epoch: 5, d_loss: 0.49018662892138953, g_loss: 3.4390353298551135\n",
            "Iter: 3120, Epoch: 5, d_loss: 0.2548506139183646, g_loss: 2.369651017883859\n",
            "Iter: 3130, Epoch: 5, d_loss: 0.1667312482323193, g_loss: 3.023580279352465\n",
            "Iter: 3140, Epoch: 5, d_loss: 0.18309135899794357, g_loss: 3.3536674656101213\n",
            "Iter: 3150, Epoch: 5, d_loss: 0.1245054416045407, g_loss: 1.9450223792392956\n",
            "Iter: 3160, Epoch: 5, d_loss: 0.38969644658240626, g_loss: 1.7366808393288613\n",
            "Iter: 3170, Epoch: 5, d_loss: 0.28588011603519525, g_loss: 2.818867006888132\n",
            "Iter: 3180, Epoch: 5, d_loss: 0.2210946124127491, g_loss: 3.621455817666094\n",
            "Iter: 3190, Epoch: 5, d_loss: 0.15240757325180224, g_loss: 3.314573677999928\n",
            "Iter: 3200, Epoch: 5, d_loss: 0.39911330479874, g_loss: 2.3131635070764265\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3210, Epoch: 5, d_loss: 0.10517279751583758, g_loss: 2.820600699409699\n",
            "Iter: 3220, Epoch: 5, d_loss: 0.3710231965839759, g_loss: 2.3062889572482264\n",
            "Iter: 3230, Epoch: 5, d_loss: 0.36381236005683903, g_loss: 2.830352352871073\n",
            "Iter: 3240, Epoch: 5, d_loss: 0.17779542457699352, g_loss: 2.9036437323555626\n",
            "Iter: 3250, Epoch: 5, d_loss: 0.2904327895140001, g_loss: 3.295774477960292\n",
            "Iter: 3260, Epoch: 5, d_loss: 0.8438181143784427, g_loss: 3.116986757943744\n",
            "Iter: 3270, Epoch: 5, d_loss: 0.5000118798640705, g_loss: 2.579493667628739\n",
            "Iter: 3280, Epoch: 5, d_loss: 0.21646614010402068, g_loss: 3.8012610695697564\n",
            "Iter: 3290, Epoch: 5, d_loss: 0.44977286900183494, g_loss: 3.306588815694358\n",
            "Iter: 3300, Epoch: 5, d_loss: 0.3126040865574221, g_loss: 2.19949176611717\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3310, Epoch: 5, d_loss: 0.2165423202479021, g_loss: 1.972085202626331\n",
            "Iter: 3320, Epoch: 5, d_loss: 0.30480870391423065, g_loss: 3.2895475409400796\n",
            "Iter: 3330, Epoch: 5, d_loss: 0.29883514001441064, g_loss: 3.185170064612791\n",
            "Iter: 3340, Epoch: 5, d_loss: 0.14802258433239562, g_loss: 2.2852398922819157\n",
            "Iter: 3350, Epoch: 5, d_loss: 0.29126540180579163, g_loss: 3.9244750741762426\n",
            "Iter: 3360, Epoch: 5, d_loss: 0.47648629273087983, g_loss: 3.805435953489652\n",
            "Iter: 3370, Epoch: 5, d_loss: 0.24032533722856925, g_loss: 2.6783700245491575\n",
            "Iter: 3380, Epoch: 5, d_loss: 0.3458313156511027, g_loss: 3.1067446070533755\n",
            "Iter: 3390, Epoch: 5, d_loss: 0.21277970368882365, g_loss: 1.9072858774572468\n",
            "Iter: 3400, Epoch: 5, d_loss: 0.27965725466771335, g_loss: 2.187170958498545\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3410, Epoch: 5, d_loss: 0.1886985139704075, g_loss: 4.035327081784466\n",
            "Iter: 3420, Epoch: 5, d_loss: 0.6103933514893882, g_loss: 3.1334305904778046\n",
            "Iter: 3430, Epoch: 5, d_loss: 0.28203840865470836, g_loss: 3.0773320861782234\n",
            "Iter: 3440, Epoch: 5, d_loss: 0.18215407482975668, g_loss: 3.090463980986842\n",
            "Iter: 3450, Epoch: 5, d_loss: 0.23824677979324702, g_loss: 3.1260346134749066\n",
            "Iter: 3460, Epoch: 5, d_loss: 0.1691130306554035, g_loss: 3.1182106531533647\n",
            "Iter: 3470, Epoch: 5, d_loss: 0.18309508323825824, g_loss: 3.2247816074526057\n",
            "Iter: 3480, Epoch: 5, d_loss: 0.14421614544486808, g_loss: 3.5564441040083263\n",
            "Iter: 3490, Epoch: 5, d_loss: 0.15286017542028985, g_loss: 4.750452408456056\n",
            "Iter: 3500, Epoch: 5, d_loss: 0.24023566151636477, g_loss: 3.0555583000659703\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3510, Epoch: 5, d_loss: 0.23110491519120718, g_loss: 2.8014326681794928\n",
            "Iter: 3520, Epoch: 5, d_loss: 0.42211740397937225, g_loss: 3.039442151766266\n",
            "Iter: 3530, Epoch: 5, d_loss: 0.2015229897687041, g_loss: 4.378969134080201\n",
            "Iter: 3540, Epoch: 5, d_loss: 0.35725079072355104, g_loss: 2.2964907474817036\n",
            "Iter: 3550, Epoch: 5, d_loss: 0.44619773677606916, g_loss: 2.3760998023294047\n",
            "Iter: 3560, Epoch: 5, d_loss: 0.1859317070600443, g_loss: 2.394724900818588\n",
            "Iter: 3570, Epoch: 5, d_loss: 0.28230528510530184, g_loss: 3.055933037169445\n",
            "Iter: 3580, Epoch: 5, d_loss: 0.21103596342459333, g_loss: 4.094420386870887\n",
            "Iter: 3590, Epoch: 5, d_loss: 0.38675513939506334, g_loss: 2.537717453712988\n",
            "Iter: 3600, Epoch: 5, d_loss: 0.23690906349850685, g_loss: 2.749803290701872\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3610, Epoch: 5, d_loss: 0.20679269033220665, g_loss: 1.7929335628537806\n",
            "Iter: 3620, Epoch: 5, d_loss: 0.22212905188173693, g_loss: 3.8820481938186187\n",
            "Iter: 3630, Epoch: 5, d_loss: 0.73864041449807, g_loss: 3.5459627850589412\n",
            "Iter: 3640, Epoch: 5, d_loss: 0.2381576537061233, g_loss: 2.7196897474562256\n",
            "Iter: 3650, Epoch: 5, d_loss: 0.20943933777047263, g_loss: 2.9692500175065537\n",
            "Iter: 3660, Epoch: 5, d_loss: 0.30857880120733094, g_loss: 4.070689264615669\n",
            "Iter: 3670, Epoch: 5, d_loss: 0.34934855149278154, g_loss: 2.921860772684791\n",
            "Iter: 3680, Epoch: 5, d_loss: 0.2819424956904617, g_loss: 3.948539754220022\n",
            "Iter: 3690, Epoch: 5, d_loss: 0.12479136514763373, g_loss: 3.048467455926022\n",
            "Iter: 3700, Epoch: 5, d_loss: 0.23832051590254053, g_loss: 2.5998301435873974\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3710, Epoch: 5, d_loss: 0.12300794657558248, g_loss: 3.1935220214156916\n",
            "Iter: 3720, Epoch: 5, d_loss: 0.32732265735924104, g_loss: 3.5187098858679677\n",
            "Iter: 3730, Epoch: 5, d_loss: 0.41127093147403815, g_loss: 2.2951672923275233\n",
            "Iter: 3740, Epoch: 5, d_loss: 0.23643942977949164, g_loss: 2.7943251142825623\n",
            "Iter: 3750, Epoch: 5, d_loss: 0.18930483452854352, g_loss: 2.7929503884920135\n",
            "Iter: 3760, Epoch: 5, d_loss: 0.4240327851830084, g_loss: 1.862015847784563\n",
            "Iter: 3770, Epoch: 5, d_loss: 0.11513742383920889, g_loss: 3.2706828073582637\n",
            "Iter: 3780, Epoch: 5, d_loss: 0.11316701374850968, g_loss: 3.0459465242863564\n",
            "Iter: 3790, Epoch: 5, d_loss: 0.15534696693496514, g_loss: 3.3920857469668566\n",
            "Iter: 3800, Epoch: 5, d_loss: 0.3200844705844367, g_loss: 2.6341763902368736\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3810, Epoch: 5, d_loss: 0.112360903740026, g_loss: 3.4687368167484687\n",
            "Iter: 3820, Epoch: 5, d_loss: 0.1870947178903037, g_loss: 3.4042606160678974\n",
            "Iter: 3830, Epoch: 5, d_loss: 0.2138388128131883, g_loss: 2.5497201201226387\n",
            "Iter: 3840, Epoch: 5, d_loss: 0.5274018618898856, g_loss: 2.231663002289091\n",
            "Iter: 3850, Epoch: 5, d_loss: 0.5886598864068434, g_loss: 3.245026141001594\n",
            "Iter: 3860, Epoch: 5, d_loss: 0.3964156785814771, g_loss: 3.793605655864691\n",
            "Iter: 3870, Epoch: 5, d_loss: 0.28843868665318223, g_loss: 2.816928438456678\n",
            "Iter: 3880, Epoch: 5, d_loss: 0.5627793335472677, g_loss: 2.0971772605993815\n",
            "Iter: 3890, Epoch: 5, d_loss: 0.19404369605945707, g_loss: 3.754766352444687\n",
            "Iter: 3900, Epoch: 6, d_loss: 0.28116577214137956, g_loss: 2.984334274046278\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 3910, Epoch: 6, d_loss: 0.2905795596174781, g_loss: 2.3510289389707246\n",
            "Iter: 3920, Epoch: 6, d_loss: 0.3354676033283013, g_loss: 3.364833472177694\n",
            "Iter: 3930, Epoch: 6, d_loss: 0.16178981880740903, g_loss: 2.257768577848273\n",
            "Iter: 3940, Epoch: 6, d_loss: 0.1494950459956999, g_loss: 4.889912789542755\n",
            "Iter: 3950, Epoch: 6, d_loss: 0.24556453190909905, g_loss: 3.065902593251483\n",
            "Iter: 3960, Epoch: 6, d_loss: 0.1699712792507734, g_loss: 1.8371367688726823\n",
            "Iter: 3970, Epoch: 6, d_loss: 0.16842004056422838, g_loss: 2.482759195794934\n",
            "Iter: 3980, Epoch: 6, d_loss: 0.18742689340682472, g_loss: 2.2971631686166716\n",
            "Iter: 3990, Epoch: 6, d_loss: 0.15982765183078806, g_loss: 3.4025298283856293\n",
            "Iter: 4000, Epoch: 6, d_loss: 0.2592421097450583, g_loss: 2.9530819590542645\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 4010, Epoch: 6, d_loss: 0.10030635286959261, g_loss: 5.134552693977008\n",
            "Iter: 4020, Epoch: 6, d_loss: 0.11836290494460233, g_loss: 2.6171413620515622\n",
            "Iter: 4030, Epoch: 6, d_loss: 0.47918473428777525, g_loss: 1.7454247484389291\n",
            "Iter: 4040, Epoch: 6, d_loss: 0.4307168928659698, g_loss: 3.433695862237181\n",
            "Iter: 4050, Epoch: 6, d_loss: 0.27840287974349776, g_loss: 3.4832511499180816\n",
            "Iter: 4060, Epoch: 6, d_loss: 0.3792257597535381, g_loss: 3.8843948161444266\n",
            "Iter: 4070, Epoch: 6, d_loss: 0.3128052099597281, g_loss: 4.803495542565476\n",
            "Iter: 4080, Epoch: 6, d_loss: 0.31792560331410086, g_loss: 2.0486309204255098\n",
            "Iter: 4090, Epoch: 6, d_loss: 0.15397401469302635, g_loss: 4.022311516872945\n",
            "Iter: 4100, Epoch: 6, d_loss: 0.10763115526031951, g_loss: 2.9435242752495885\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 4110, Epoch: 6, d_loss: 0.2481081987227305, g_loss: 2.5249390708924655\n",
            "Iter: 4120, Epoch: 6, d_loss: 0.13426203913153253, g_loss: 3.606538811912938\n",
            "Iter: 4130, Epoch: 6, d_loss: 0.3264982503976292, g_loss: 2.568414184369794\n",
            "Iter: 4140, Epoch: 6, d_loss: 0.18258289997110724, g_loss: 2.3435861010467125\n",
            "Iter: 4150, Epoch: 6, d_loss: 0.3815397088079716, g_loss: 2.7249342782967467\n",
            "Iter: 4160, Epoch: 6, d_loss: 0.07879680816903008, g_loss: 2.9963553642404523\n",
            "Iter: 4170, Epoch: 6, d_loss: 0.14992953717566643, g_loss: 3.6740584560167955\n",
            "Iter: 4180, Epoch: 6, d_loss: 0.12768714153874286, g_loss: 3.215961475213347\n",
            "Iter: 4190, Epoch: 6, d_loss: 0.22557387202034884, g_loss: 2.5033586714504703\n",
            "Iter: 4200, Epoch: 6, d_loss: 0.5154734665358686, g_loss: 1.768105845186834\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/GAN/Discriminator.npy\n",
            "Iter: 4210, Epoch: 6, d_loss: 0.21612794493213405, g_loss: 2.548218702209999\n",
            "Iter: 4220, Epoch: 6, d_loss: 0.5053176472554268, g_loss: 3.4023598745864545\n",
            "Iter: 4230, Epoch: 6, d_loss: 0.15858389101698253, g_loss: 3.6190351985451685\n",
            "Iter: 4240, Epoch: 6, d_loss: 0.06659904995372694, g_loss: 4.968276499652168\n",
            "Iter: 4250, Epoch: 6, d_loss: 0.20929332582015486, g_loss: 3.6612368768746824\n",
            "Iter: 4260, Epoch: 6, d_loss: 0.14207693068491328, g_loss: 3.380162901224928\n",
            "Iter: 4270, Epoch: 6, d_loss: 0.25380371387274403, g_loss: 2.044750311874744\n",
            "Iter: 4280, Epoch: 6, d_loss: 0.3397891890010304, g_loss: 2.425246342454508\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}