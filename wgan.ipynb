{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "wgan.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srirambandi/GAN/blob/master/wgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk-FsI3wcrMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e4b8d07-0fb0-4ecc-b30d-8b69dad06192"
      },
      "source": [
        "# when running in colab notebooks, first install library\n",
        "!pip install import-ai\n",
        "# upload respective dataset manually from examples directory of the library or download as below\n",
        "!apt install subversion\n",
        "!svn checkout https://github.com/srirambandi/ai/trunk/examples/MNIST"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting import-ai\n",
            "  Downloading https://files.pythonhosted.org/packages/16/c9/61a99b75a3ccd70ddd207ea12e28b9baa17feba9fffb2d7181d03d1b8147/import_ai-1.3.11-py3-none-any.whl\n",
            "Collecting graphviz>=0.14\n",
            "  Downloading https://files.pythonhosted.org/packages/83/cc/c62100906d30f95d46451c15eb407da7db201e30f42008f3643945910373/graphviz-0.14-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.6/dist-packages (from import-ai) (1.18.5)\n",
            "Installing collected packages: graphviz, import-ai\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.14 import-ai-1.3.11\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1\n",
            "Suggested packages:\n",
            "  db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 subversion\n",
            "0 upgraded, 5 newly installed, 0 to remove and 59 not upgraded.\n",
            "Need to get 2,237 kB of archives.\n",
            "After this operation, 9,910 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libapr1 amd64 1.6.3-2 [90.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libaprutil1 amd64 1.6.1-2 [84.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libserf-1-1 amd64 1.3.9-6 [44.4 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libsvn1 amd64 1.9.7-4ubuntu1 [1,183 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 subversion amd64 1.9.7-4ubuntu1 [834 kB]\n",
            "Fetched 2,237 kB in 2s (1,390 kB/s)\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../libapr1_1.6.3-2_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.6.3-2) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../libaprutil1_1.6.1-2_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-2) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../libserf-1-1_1.3.9-6_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../libsvn1_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../subversion_1.9.7-4ubuntu1_amd64.deb ...\n",
            "Unpacking subversion (1.9.7-4ubuntu1) ...\n",
            "Setting up libapr1:amd64 (1.6.3-2) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-2) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-6) ...\n",
            "Setting up libsvn1:amd64 (1.9.7-4ubuntu1) ...\n",
            "Setting up subversion (1.9.7-4ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "A    MNIST/test.npy\n",
            "A    MNIST/train.npy\n",
            "Checked out revision 120.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSHzfWABegwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a75f3d01-cd99-4481-fdfe-ad3f0880c9a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bql-uXp0cqPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ai\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unwx_Hc1cqPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_dim = 100\n",
        "gf_dim = 64\n",
        "df_dim = 64"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQzdpXIpcyL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ai.manual_seed(2357)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBgfrdNzcqPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_generator(m):\n",
        "    train_dict = np.load('MNIST/train.npy', allow_pickle=True)\n",
        "    test_dict = np.load('MNIST/test.npy', allow_pickle=True)\n",
        "    data = np.concatenate([train_dict.item()['data'], test_dict.item()['data']])\n",
        "    data = data.transpose(1, 2, 0)   # making data batch-last\n",
        "    data = data.reshape(1, *data.shape) / 127.5 - 1.   # adding channel dimension and normalizing data\n",
        "    epoch = 8\n",
        "    \n",
        "    while True:\n",
        "        epoch += 1\n",
        "        for batch in range(int(data.shape[-1] / m)):\n",
        "            yield data[...,batch * m:(batch + 1) * m], epoch"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juS9j_gScqPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(ai.Module):\n",
        "    def __init__(self):\n",
        "        self.g_fc = ai.Linear(z_dim, 8*gf_dim * 2 * 2)\n",
        "        self.g_bn1 = ai.BatchNorm((8*gf_dim, 2, 2))\n",
        "        self.g_deconv1 = ai.ConvTranspose2d(8*gf_dim, 4*gf_dim, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        self.g_bn2 = ai.BatchNorm((4*gf_dim, 4, 4))\n",
        "        self.g_deconv2 = ai.ConvTranspose2d(4*gf_dim, 2*gf_dim, kernel_size=5, stride=2, padding=2, a=0)\n",
        "        self.g_bn3 = ai.BatchNorm((2*gf_dim, 7, 7))\n",
        "        self.g_deconv3 = ai.ConvTranspose2d(2*gf_dim, gf_dim, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        self.g_bn4 = ai.BatchNorm((gf_dim, 14, 14))\n",
        "        self.g_deconv4 = ai.ConvTranspose2d(gf_dim, 1, kernel_size=5, stride=2, padding=2, a=1)\n",
        "        \n",
        "    def forward(self, z):\n",
        "        o1 = ai.G.reshape(self.g_fc(z), (8*gf_dim, 2, 2))\n",
        "        o2 = ai.G.relu(self.g_bn1(o1))\n",
        "        o3 = ai.G.relu(self.g_bn2(self.g_deconv1(o2)))\n",
        "        o4 = ai.G.relu(self.g_bn3(self.g_deconv2(o3)))\n",
        "        o5 = ai.G.relu(self.g_bn4(self.g_deconv3(o4)))\n",
        "        fake_image = ai.G.tanh(self.g_deconv4(o5))\n",
        "        return fake_image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPElqs6gcqPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(ai.Module):\n",
        "    def __init__(self):\n",
        "        self.d_conv1 = ai.Conv2d(1, 64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_conv2 = ai.Conv2d(64, 2*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn1 = ai.BatchNorm((2*64, 7, 7))\n",
        "        self.d_conv3 = ai.Conv2d(2*64, 3*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn2 = ai.BatchNorm((3*64, 4, 4))\n",
        "        self.d_conv4 = ai.Conv2d(3*64, 4*64, kernel_size=5, stride=2, padding=2)\n",
        "        self.d_bn3 = ai.BatchNorm((4*64, 2, 2))\n",
        "        self.d_fc = ai.Linear(1024, 1)\n",
        "        \n",
        "    def forward(self, image):\n",
        "        o1 = ai.G.lrelu(self.d_conv1(image))\n",
        "        o2 = ai.G.lrelu(self.d_bn1(self.d_conv2(o1)))\n",
        "        o3 = ai.G.lrelu(self.d_bn2(self.d_conv3(o2)))\n",
        "        o4 = ai.G.lrelu(self.d_bn3(self.d_conv4(o3)))\n",
        "        o5 = self.d_fc(o4)\n",
        "        return o5"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50IAOgjXcqPr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "8d8050a0-c307-4dc6-ca97-6324990cb3ed"
      },
      "source": [
        "generator = Generator()\n",
        "critic = Critic()\n",
        "generator.load('/content/drive/My Drive/WGAN/Generator.npy')\n",
        "critic.load('/content/drive/My Drive/WGAN/Critic.npy')\n",
        "print(generator)\n",
        "print(critic)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model...\n",
            "Successfully loaded model from /content/drive/My Drive/WGAN/Generator.npy\n",
            "loading model...\n",
            "Successfully loaded model from /content/drive/My Drive/WGAN/Critic.npy\n",
            "Generator(\n",
            "  g_fc: Linear(input_features=100, output_features=2048, bias=True)\n",
            "  g_bn1: BatchNorm((512, 2, 2), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv1: ConvTranspose2d(512, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            "  g_bn2: BatchNorm((256, 4, 4), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv2: ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(0, 0), bias=True)\n",
            "  g_bn3: BatchNorm((128, 7, 7), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv3: ConvTranspose2d(128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            "  g_bn4: BatchNorm((64, 14, 14), axis=-1, momentum=0.9, bias=True)\n",
            "  g_deconv4: ConvTranspose2d(64, 1, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), a=(1, 1), bias=True)\n",
            ")\n",
            "Critic(\n",
            "  d_conv1: Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_conv2: Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn1: BatchNorm((128, 7, 7), axis=-1, momentum=0.9, bias=True)\n",
            "  d_conv3: Conv2d(128, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn2: BatchNorm((192, 4, 4), axis=-1, momentum=0.9, bias=True)\n",
            "  d_conv4: Conv2d(192, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=True)\n",
            "  d_bn3: BatchNorm((256, 2, 2), axis=-1, momentum=0.9, bias=True)\n",
            "  d_fc: Linear(input_features=1024, output_features=1, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYbbyJZBcqPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.00005\n",
        "c = 0.01\n",
        "g_optim = ai.Optimizer(generator.parameters(), optim_fn='RMSProp', lr=lr)\n",
        "c_optim = ai.Optimizer(critic.parameters(), optim_fn='RMSProp', lr=lr)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRh7R4j1cqP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it, epoch = 0, 0\n",
        "m = 64   # batch size\n",
        "n_critic = 5   # number of critic updates per generator update\n",
        "c = 0.01  # clip value for critic weigthts after each update\n",
        "\n",
        "batch_size = ai.Parameter((1, 1), init_ones=True, constant=float(m), eval_grad=False)   # batch size 'm' as a Parameter constant\n",
        "neg_ones = ai.Parameter((1, 1), init_ones=True, constant=-1.0, eval_grad=False)\n",
        "\n",
        "# real images data generator\n",
        "data = data_generator(m)\n",
        "\n",
        "sample_z = np.random.uniform(-1, 1, (z_dim, m))\n",
        "sampled_images = np.load('/content/drive/My Drive/WGAN/sampled_images.npy')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLpbkD0LcqP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampler(sampled_images):\n",
        "    ai.G.grad_mode = False\n",
        "\n",
        "    # generate images like real data\n",
        "    fake_images = generator.forward(sample_z).data\n",
        "    fake_images = (fake_images + 1.) / 2.\n",
        "\n",
        "    if sampled_images is not None:\n",
        "        sampled_images = np.concatenate([sampled_images, fake_images], axis=-1)\n",
        "    else:\n",
        "        sampled_images = fake_images\n",
        "    \n",
        "    ai.G.grad_mode = True\n",
        "\n",
        "    return sampled_images"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKaygbefcqQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "717164f1-4209-4334-eeef-ace9b4fea91f"
      },
      "source": [
        "for it in range(1601, 10000):\n",
        "\n",
        "    # freeze generator before optimizing critic\n",
        "    for p in generator.parameters():\n",
        "        p.eval_grad = False\n",
        "\n",
        "    # critic_iter = 100 if it < 25 or it%500 == 0 else n_critic\n",
        "\n",
        "    # training critic to identify real/fake data\n",
        "    for _ in range(n_critic):\n",
        "\n",
        "        real_images, epoch = data.__next__()\n",
        "        if (real_images.shape[-1] != m):\n",
        "            continue\n",
        "\n",
        "        c_loss_real = critic.forward(real_images)\n",
        "\n",
        "        z = np.random.randn(z_dim, m)\n",
        "        fake_images = generator.forward(z)\n",
        "\n",
        "        c_loss_fake = critic.forward(fake_images)\n",
        "\n",
        "        # add the above two losses and reduce their mean\n",
        "        c_loss = c_loss_fake - c_loss_real\n",
        "        c_loss = ai.G.sum(c_loss)\n",
        "        c_loss = c_loss / batch_size\n",
        "        c_loss.grad = np.ones(c_loss.shape)\n",
        "\n",
        "        c_loss.backward()\n",
        "        c_optim.step()\n",
        "        c_optim.zero_grad()\n",
        "\n",
        "        ai.clip_grad_value(critic.parameters(), c)\n",
        "\n",
        "    # unfreeze generator\n",
        "    for p in generator.parameters():\n",
        "        p.eval_grad = True\n",
        "\n",
        "    # training generator to fool critic with fake data\n",
        "    z = np.random.uniform(-1, 1, (z_dim, m))\n",
        "    fake_images = generator.forward(z)\n",
        "\n",
        "    g_loss = critic.forward(fake_images)\n",
        "    g_loss = ai.G.sum(g_loss)\n",
        "    g_loss = g_loss / batch_size\n",
        "    g_loss = neg_ones * g_loss\n",
        "    g_loss.grad = np.ones(g_loss.shape)\n",
        "\n",
        "    g_loss.backward()\n",
        "    g_optim.step()\n",
        "    g_optim.zero_grad()\n",
        "    c_optim.zero_grad()\n",
        "\n",
        "    if it%10 == 0:\n",
        "        print('Iter: {}, Epoch: {}, c_loss: {}, g_loss: {}'.format(it, epoch, c_loss.data[0, 0], g_loss.data[0, 0]))\n",
        "    \n",
        "    if it%100 == 0:\n",
        "        sampled_images=sampler(sampled_images)\n",
        "        np.save('/content/drive/My Drive/WGAN/sampled_images.npy', sampled_images)\n",
        "        generator.save('/content/drive/My Drive/WGAN/Generator.npy')\n",
        "        critic.save('/content/drive/My Drive/WGAN/Critic.npy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using RMSProp\n",
            "using RMSProp\n",
            "Iter: 1610, Epoch: 9, c_loss: -215.44054595552703, g_loss: 117.43643948892955\n",
            "Iter: 1620, Epoch: 9, c_loss: -218.90042109670495, g_loss: 119.71618076922859\n",
            "Iter: 1630, Epoch: 9, c_loss: -223.26048732516497, g_loss: 125.87758379667203\n",
            "Iter: 1640, Epoch: 9, c_loss: -212.27170692221114, g_loss: 121.6640273847116\n",
            "Iter: 1650, Epoch: 9, c_loss: -224.64036433880872, g_loss: 127.63385920774866\n",
            "Iter: 1660, Epoch: 9, c_loss: -221.7600993908171, g_loss: 122.04462021984887\n",
            "Iter: 1670, Epoch: 9, c_loss: -230.0265560859551, g_loss: 117.63620357876624\n",
            "Iter: 1680, Epoch: 9, c_loss: -219.11592850791192, g_loss: 128.81328420603802\n",
            "Iter: 1690, Epoch: 9, c_loss: -227.51576780926334, g_loss: 119.16979566115273\n",
            "Iter: 1700, Epoch: 9, c_loss: -243.43268006362237, g_loss: 128.9579509807634\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Critic.npy\n",
            "Iter: 1710, Epoch: 9, c_loss: -224.05293203604126, g_loss: 136.79908186863653\n",
            "Iter: 1720, Epoch: 9, c_loss: -239.86181098676494, g_loss: 132.1933017073888\n",
            "Iter: 1730, Epoch: 9, c_loss: -250.34850484957897, g_loss: 127.6022737850965\n",
            "Iter: 1740, Epoch: 9, c_loss: -237.79041827864546, g_loss: 132.202291151149\n",
            "Iter: 1750, Epoch: 9, c_loss: -240.31049377990286, g_loss: 131.22462619520448\n",
            "Iter: 1760, Epoch: 9, c_loss: -246.3376060110481, g_loss: 130.49554472399757\n",
            "Iter: 1770, Epoch: 9, c_loss: -240.825790416897, g_loss: 141.08870797074385\n",
            "Iter: 1780, Epoch: 9, c_loss: -230.54084116564675, g_loss: 123.72887294779594\n",
            "Iter: 1790, Epoch: 9, c_loss: -235.66574887070487, g_loss: 122.0902096503996\n",
            "Iter: 1800, Epoch: 9, c_loss: -246.12283284921693, g_loss: 129.71388024920986\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Critic.npy\n",
            "Iter: 1810, Epoch: 9, c_loss: -206.85488041297617, g_loss: 152.24001058242652\n",
            "Iter: 1820, Epoch: 10, c_loss: -252.69101801840029, g_loss: 125.93248226932606\n",
            "Iter: 1830, Epoch: 10, c_loss: -266.95164109836645, g_loss: 157.04620919929988\n",
            "Iter: 1840, Epoch: 10, c_loss: -261.1639862628216, g_loss: 149.1271490708887\n",
            "Iter: 1850, Epoch: 10, c_loss: -265.10129926308076, g_loss: 131.33634427704717\n",
            "Iter: 1860, Epoch: 10, c_loss: -271.1012928803325, g_loss: 150.99431844614236\n",
            "Iter: 1870, Epoch: 10, c_loss: -270.744243420934, g_loss: 154.3781864279504\n",
            "Iter: 1880, Epoch: 10, c_loss: -261.80217902625805, g_loss: 149.8880150159739\n",
            "Iter: 1890, Epoch: 10, c_loss: -251.1492091793974, g_loss: 149.25080605520657\n",
            "Iter: 1900, Epoch: 10, c_loss: -276.8095772830663, g_loss: 147.52675155588594\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Critic.npy\n",
            "Iter: 1910, Epoch: 10, c_loss: -284.4075559670317, g_loss: 150.79789978976265\n",
            "Iter: 1920, Epoch: 10, c_loss: -274.7610767215668, g_loss: 152.3338381415457\n",
            "Iter: 1930, Epoch: 10, c_loss: -285.0049637179041, g_loss: 149.79642900310967\n",
            "Iter: 1940, Epoch: 10, c_loss: -264.6218796985784, g_loss: 155.72473696896853\n",
            "Iter: 1950, Epoch: 10, c_loss: -278.2304609554772, g_loss: 154.88047431195707\n",
            "Iter: 1960, Epoch: 10, c_loss: -287.5026013510391, g_loss: 162.31244194683296\n",
            "Iter: 1970, Epoch: 10, c_loss: -277.39292187052774, g_loss: 152.85744921395656\n",
            "Iter: 1980, Epoch: 10, c_loss: -291.25744187185205, g_loss: 156.44056115134444\n",
            "Iter: 1990, Epoch: 10, c_loss: -290.97092932370384, g_loss: 160.06405782896755\n",
            "Iter: 2000, Epoch: 10, c_loss: -269.4210765050952, g_loss: 163.0618480591968\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Generator.npy\n",
            "saving model...\n",
            "Successfully saved model in /content/drive/My Drive/WGAN/Critic.npy\n",
            "Iter: 2010, Epoch: 10, c_loss: -295.27899797454813, g_loss: 150.67245559811965\n",
            "Iter: 2020, Epoch: 10, c_loss: -298.76736637429264, g_loss: 155.2079491370589\n",
            "Iter: 2030, Epoch: 10, c_loss: -287.5599652744722, g_loss: 164.501592329817\n",
            "Iter: 2040, Epoch: 11, c_loss: -304.43427855082246, g_loss: 175.8143889836475\n",
            "Iter: 2050, Epoch: 11, c_loss: -309.8852291210364, g_loss: 169.64867919163228\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}